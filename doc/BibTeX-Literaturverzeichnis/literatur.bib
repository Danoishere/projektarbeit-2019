@MANUAL{mirrorcle_userguide,
	AUTHOR = {Dr. Veljko Milanović and Abhishek Kasturi},
	TITLE = {Mirrorcle Technologies, Inc. PicoAmp 4.0 User Guide},
	ORGANIZATION = {Mirrorcle Technologies},
	YEAR = {2012},
	MONTH = {April}
}

@MANUAL{microchip_spi,
	ORGANIZATION = {Microchip Technology},
	TITLE = {Section 23. Serial Peripheral Interface (SPI)},
	YEAR = {2011}
}

@MANUAL{analog_devices_dac,
	ORGANIZATION = {Analog Devices},
	TITLE = {Quad, 12-/14-/16-Bit nanoDACs with 5 ppm/°C On-Chip Reference},
	YEAR = {2008}
}

@article{mnih2013playing,
	added-at = {2017-12-07T19:44:53.000+0100},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	biburl = {https://www.bibsonomy.org/bibtex/24140efabf4cab720d020e0e68f1451f6/lukasw},
	interhash = {78966703f649bae69a08a6a23a4e8879},
	intrahash = {4140efabf4cab720d020e0e68f1451f6},
	journal = {arXiv preprint arXiv:1312.5602},
	keywords = {atari final},
	timestamp = {2017-12-07T19:44:53.000+0100},
	title = {Playing atari with deep reinforcement learning},
	url = {https://arxiv.org/pdf/1312.5602.pdf},
	year = 2013
}

@article{a3c,
	Author = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	Title = {Asynchronous Methods for Deep Reinforcement Learning},
	Year = {2016},
	Eprint = {arXiv:1602.01783},
	Howpublished = {ICML 2016},
}

@article{marlsurvey,
	Author = {Pablo Hernandez-Leal and Bilal Kartal and Matthew E. Taylor},
	Title = {A Survey and Critique of Multiagent Deep Reinforcement Learning},
	Year = {2018},
	Eprint = {arXiv:1810.05587},
	Doi = {10.1007/s10458-019-09421-1},
}

@misc{marltraffica3c,
	Author = {Giulio Bacchiani and Daniele Molinari and Marco Patander},
	Title = {Microscopic Traffic Simulation by Cooperative Multi-agent Deep Reinforcement Learning},
	Year = {2019},
	Eprint = {arXiv:1903.01365},
}
	
	
@article {alphazero,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/362/6419/1140},
	eprint = {https://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}

@misc{hideandseek,
	Author = {Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
	Title = {Emergent Tool Use From Multi-Agent Autocurricula},
	Year = {2019},
	Eprint = {arXiv:1909.07528},
}

@inproceedings{policygradient,
	author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
	series = {NIPS'99},
	year = {1999},
	location = {Denver, CO},
	pages = {1057--1063},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=3009657.3009806},
	acmid = {3009806},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 


@Article{tdlearning,
	author="Sutton, Richard S.",
	title="Learning to predict by the methods of temporal differences",
	journal="Machine Learning",
	year="1988",
	month="Aug",
	day="01",
	volume="3",
	number="1",
	pages="9--44",
	abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
	issn="1573-0565",
	doi="10.1007/BF00115009",
	url="https://doi.org/10.1007/BF00115009"
}


@thesis{
	explorationexploitation,
	author ="Lukas Rusch",
	title ="Exploration-Exploitation Trade-off in Deep Reinforcement Learning",

	url = "https://pub.tik.ee.ethz.ch/students/2018-FS/BA-2018-15.pdf",
	type = "Bachelor's thesis",
	institution = "Distributed Computing Group	Computer Engineering and Networks Laboratory	ETH Zürich",
}




