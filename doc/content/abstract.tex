%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage für Windows ProTeXt mit TexMakerX
% Title       : 
% File        : abstract.tex Rev. 00
% Date        : 23.4.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\chapter*{Zusammenfassung}\label{chap.zusammenfassung}

%1. Einleitung («Problemstellung»): Definiert die Problematik und begründet die Relevanz der Arbeit. Die Situation (eine problematische Situation oder technische Problematik) und die wissenschaftliche/fachliche Untersuchungs-Fragestellung (folgt logischer- weise aus der festgestellten Problematik) werden kurz beschrieben.
Die steigende Anzahl der Pendler bring die Kapazität des Schienennetzes der Schweizerischen Bundesbahnen SBB immer mehr an seine Grenzen. Da der Ausbau der Infrastruktur nicht mit den Passagierzahlen mithalten kann, bemüht sich die SBB, mehr Züge in dichteren Abständen auf ihr Netz zu bringen.
Für die Planung des dichteren Zugverkehrs experimentiert die SBB auch mit Technologien des maschinellen Lernens, insbesondere mit Reinforcement Learning (RL). Dabei soll ein Algorithmus selbstständig Strategien erlernen, um das gegebene Problem zu lösen. In diesem Fall handelt es sich dabei um das Steuern von Zügen, welche möglichst zeitnah zu ihrem Ziel geführt werden sollen. Kollisionen gilt es dabei zu verhindern und potenzielle Hindernisse wie defekte Züge müssen umfahren werden. Auch können Züge mit verschiedenen Geschwindigkeiten unterwegs sein, was bei der Planung berücksichtigt werden soll. Diese Arbeit ist dabei ein Beitrag zur sogenannten Flatland Challenge, einem Wettbewerb der SBB und der Crowdsourcing-Plattform AICrowd, bei welchem das beschriebene Problem mittels einer zur Verfügung gestellten Simulation des Schienennetzes gelöst werden soll. Der Wettbewerb ist aufgeteilt in zwei Runden mit wachsendem Schwierigkeitslevel.
Diese Schienensimulation erlaubt es, eigene Lösungen für das Problem zu trainieren und evaluieren. Die Schwierigkeit dabei ist, die Züge so zu steuern, dass sie sich auch in komplexen Situationen nicht blockieren. Dazu ist primär die Zusammenarbeit der Züge von grosser Bedeutung.
%3. Vorgehen («Problembehandlung»): Informiert über das Vorgehen und über die Untersuchungsanlage.
Der präsentierte Lösungsansatz verwendet den Asynchronous Advantage Actor Critic Algorithmus (A3C), einer der derzeit besten RL-Algorithmen, welcher verteiltes Lernen ermöglicht. Die präsentierte Ansatz orientiert sich dabei stark an der von Stephan Huschauer präsentierten Ausgangslösung.
Nach der ersten lauffähigen Version wurde der Ansatz nach und nach mit unterschiedlichen Features erweitert, wie beispielsweise einem veränderten Kontrollmechanismus der Züge, Curriculum Learning oder verteiltem Lernen über mehrere Rechner.
Um den Fortschritt zu quantifizieren wurden für sämtliche Änderungen Experimente durchgeführt. Durch das Hinzufügen unterschiedlicher Features konnte eine signifikante Verbesserung der Performanz verzeichnet werden. In Runde 1 war es dabei möglich, die Ankunftsrate von den 16.6\% der Ausgangslösung auf 48.9\% zu verbessern.
Für die anspruchsvolleren Eisenbahnnetze der 2. Runde, für welche keine Ausgangslösung existiert, wurden Dank weiteren Verbesserungen ebenfalls ein respektables Resultat erzielt werden. Die präsentierte Lösung konnte eine Ankunftsrate von 29.1\% erreichen, was zum Evaluierungszeitpunkt dem 4. Platz von insgesamt 24 teilnehmenden Teams entspricht. Für einen praktischen Einsatz ist die präsentierte Lösung jedoch noch nicht geeignet, dies bedarf weitere Forschung.
\newpage
\thispagestyle{empty}
\chapter*{Abstract}\label{abstract}
Due to the increasing number of commuters in recent years, the consolidation of rail traffic on the Swiss Federal Railways (SBB) existing rail network has a high priority.
The reduction of delays caused by technical faults in the infrastructure or on trains is just as relevant.
Therefore, in this thesis we deal with the control of complex train traffic systems by means of machine learning, or more precisely reinforcement learning (RL).
Swiss Federal Railways is indirectly our project partner, as they have put out a challenge on this topic on AICrowd.
SBB provides the simulation environment Flatland in cooperation with AICrowd. Flatland can be used to simulate a complex rail network with which trains controlled by artificial intelligence can interact.
The problem to be solved is a collaborative multi-agent problem. The goal is that all trains arrive at their destination station as quickly as possible and do not block each other. When a blockade occurs, it usually involves a chain reaction. As a result, many trains do not arrive at all.
We implemented our solution with the Asynchronous Advantage Actor Critic Algorithm (A3C), one of the currently best RL algorithms, which enables distributed learning.
After the first executable version, we gradually expanded our stand with various features such as long short-term memory, curriculum learning or distributed learning.
We constantly controlled our changes with experiments to ensure progress.
By adding different features we were able to achieve a result of 48.9\% in round 1.
In round 2 we changed the action space, which significantly reduced the number of queries to the neural network. This greatly improved the runtime and learning progress.
By using rules that prevent special cases, we were able to achieve an arrival probability of 29.1\%, which currently corresponds to 4th place out of 24 participating teams on AICrowd.


\chapter*{Preface}\label{preface}
We would like to give special thanks to:
\begin{itemize}
    \item Andreas Weiler and Thilo Stadelmann for their great support during this work, for their helpful tips and for pushing us into the right direction.\\
    We are grateful for the opportunity to dive deep into the field of reinforcement learning as part of this project.
    \item Remo Maurer for being very generous with providing computing infrastructure, especially the infrastructure test server.
\end{itemize}

