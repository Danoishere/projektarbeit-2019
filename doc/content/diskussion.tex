%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage f√ºr Windows ProTeXt mit TexMakerX
% Title       : 
% File        : diskussion.tex Rev. 00
% Date        : 7.5.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion and Outlook}
\label{chap.diskussion}
\chaptermark{Discussion}
\section{Review of the Application of Reinforcement Learning}\label{discussion_rl}
While trying to solve the Flatland challenge with RL, we discovered a limitation of policy-gradient based methods in high-consequence environments like Flatland. 
We call Flatland a high-consequence environment because taking a bad action quickly leads to a chain reaction of unresolvable situations. Policy-based RL uses a possibility distribution over all available actions. This leads to the problem that even if the best action is taken with a possibility of 90\%, the remainig (probably non-beneficial actions) have a combined chance of 10\% to be selected. For 10 agents with such a possibility distribution, there is already a 65.1\% chance that one of the agents takes a non-beneficial action and creates a chain of problems. Just converting this probability distribution into a deterministic policy by using the $\mathcal{argmax}$ over the possibility distribution does not solve the problem due to some situations in which the agent is not sure what to do and therefore assigns similar possibilities to the actions. The algorithm then relies on its stochasticity to try all available actions. It would be therefore interresting to experiment with value-based RL algorithms to observe, if such algorithms can overcome the described problem that policy-methods have. In most popular use-cases of RL such as Atari games, it will suffice to just select a good action and not nescessarily the best. This is different in Flatland and should therefore be adressed.

\section{Practicability in a Real World Scenario}\label{discussion_real_world}
While we were able to greatly improve the performance of the presented solution compared to the given baseline, it is still nowhere near practical applicability. While the presented evaluation tasks probably do not represent the real world density on a rail network, also with a lower volume of traffic, train traffic would require more robust solutions with the primary objective of finding a solution for every train to reach its destination instead of optimizing the performance of a single agent.

\section{Ideas for Future Research}\label{discussion_research}
We think that in order to further improve performance, the problem would need to be formulated in a different way. While the research presented in this work was mainly focused on treating the trains as agents in a multi-agent reinforcement learning problem, it might be an interresting approach to instead introduce a centrally planning agent that takes over all planning in advance. Especially in an simulated environment like Flatland with perfect information, we think that upfront planning would have potential to take full advantage of the available data and the possibility to iterativly plan the upcoming steps. With all planning done by a single agent, it would also remove the requirement for communication. While we could show in \autoref{agent_communication} that it is possible to learn communication protocols between agents, we think that in a less constructed example the convergence towards a usable communication protocol might be too slow to actually use it in real-world training with many agents.\\
Combined with the possibilty of a centrally planning agent, we think that converting the rail network into a logical graph would enable any training algorithm to perform better without the need for respecting the tile-based architecture of the Flatland environment. In \autoref{reduced_action_space}, we already take a step in this direction by reducing the actions required for each agent. 

