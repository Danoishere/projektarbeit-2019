%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage f√ºr Windows ProTeXt mit TexMakerX
% Title       : 
% File        : diskussion.tex Rev. 00
% Date        : 7.5.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion and Outlook}
\label{chap.diskussion}
\chaptermark{Discussion}
\section{Review of the Application of Reinforcement Learning}\label{discussion_rl}
While trying to solve the Flatland challenge with RL, we discovered a limitation of policy gradient-based methods in high-consequence environments like Flatland.
We call Flatland a high-consequence environment because taking a bad action quickly leads to a chain reaction of unresolvable situations. Policy based RL uses a probability distribution over all available actions. The difficulty of combining a high-consequence environment with a stochastic policy can be nicely shown in an example: We consider a situation where the best action is taken with a probability of 90\%, the remaining (probably non-beneficial actions) have a combined chance of 10\% to be selected. For 10 agents with such a probability distribution, there is already a 65.1\% chance that one of the agents takes a non-beneficial action and creates a chain reaction of problems. Just converting this probability distribution into a deterministic policy by using the $\mathcal{argmax}$ over the probability distribution does not solve the problem due to some situations in which the agent is not sure what to do and therefore assigns similar probabilities to the actions. The algorithm then relies on its stochasticity to try all available actions. Therefore it would be interesting to experiment with value-based RL algorithms to observe, if such algorithms can overcome the described problem that policy-methods have. In most popular use-cases of RL such as Atari games, it will suffice to just select a good action and not necessarily the best. This is different in Flatland and should therefore be addressed.

\section{Practicability in a Real-World Scenario}\label{discussion_real_world}
While we were able to greatly improve the performance of the presented solution compared to the given baseline, it is still nowhere near practical applicability. While the presented evaluation tasks probably do not represent the real-world density on a rail network, also with a lower volume of traffic, train traffic would require more robust solutions with the primary objective of finding a solution for every train to reach its destination instead of optimizing the performance of a single agent.

\section{Ideas for Future Research}\label{discussion_research}
We think that in order to further improve performance, the problem would need to be formulated in a different way. While the research presented in this work is mainly focused on treating the trains as agents in a multi-agent reinforcement learning problem, it might be an interesting approach to introduce a central planning agent that takes over all planning in advance. Especially in a simulated environment like Flatland with perfect information, we think that upfront planning would have the potential to take full advantage of the available data and the possibility to iteratively plan the upcoming steps. With all planning done by a single agent, it would also remove the requirement for communication. While we could show in \autoref{agent_communication} that it is possible to learn communication protocols between agents, we think that in a less constructed example the convergence towards a usable communication protocol might be too slow to actually use it in real-world training with many agents.\\
Combined with the possibility of a centrally planning agent, we think that converting the rail network into a logical graph would enable any training algorithm to perform better without the need for respecting the tile-based architecture of the Flatland environment. In \autoref{reduced_action_space}, we already take a step in this direction by reducing the actions required for each agent.\\
An alternative to the centralized planning approach could be something similar to the solution presented by Ephrati and Rosenschein in \cite{Ephrati1993MultiAgentPA}. They propose to plan on a local level and merge the locally found solutions into a global plan. Using the presented subgoals it could be possible to resolve local conflicts and iteratively move into a direction of a global solution.