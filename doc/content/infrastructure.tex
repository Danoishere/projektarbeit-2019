\chapter{Infrastructure}\label{chap.infrastruktur}

\section{Infrastructure}\label{infrastructure}
We used various computers and servers to train our model. Most of the time did we train on a test environment server of the ZHAW School of Engineering \cite{zhaw}.
The server had a total of 56 CPU cores and 721Gb memory. We connect 3 Openstack machines with 8 CPU cores each to this server, to increase our training. \\
The flatland environment was running on each cpu core and the results where sent to a webserver which updated the model and sent the new neuronal network weights back to the clients. \\
The reason why we used CPU cores over GPU performance is that the reinforcement algorithm A3C which we used performs better on CPUs instead of GPUs. \\

%TODO: Architektur Bild Server

\subsection{Distributed training}
We decided during round 1 of the Flatland challenge to change the training of the neuronal network to a distributed approach. \\
We used the server with the most memory and cores to run a web server, which distributed the neuronal network, the observations and also certain input parameters at training start. \\
The workers, all other CPUs on which an instance of the Flatland environment was running, received the files and compiled them into c code, to execute them.\\
The general cycle for each worker is as following:
\begin{enumerate}
    \item Get current model from web server
    \item Execute episode
    \item Get updated model from web server
    \item Calculate gradient and upload weights
\end{enumerate}


