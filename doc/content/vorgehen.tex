
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage f√ºr Windows ProTeXt mit TexMakerX
% Title       : 
% File        : vorgehen.tex Rev. 00
% Date        : 7.5.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Approach and Methodology}\label{chap.vorgehen}
\section{Basic Considerations}\label{basic_cons}
As described under \autoref{baseline}, our work is based on the work of S. Huschauer \cite{flatlandstephan}. We take the idea of using the A3C algorithm to solve the flatland problem and try various modifications in an attempt to improve its performance.\\
To do so, we identify parts of the implementation where we suspect possibility for improvement. We proceed by giving an idea, what we want to achieve by changing the specified part, followed by an experiment to either prove or disprove our hypothesis.\\
For training purposes, we started by reimplementing the algorithm by ourselfes. This enabled us from the beginning to gain a deeper understanding of how the algorithm works and where we could find possible areas for improvement. From there, we iteratively added these potential improvements to later compare them against the version without this feature.
Our work can be categorized into a section about RL for flatland and a section about infrastructure and scaling up the training process.
\subsection*{Reproducability in Reinforcement Learning}\label{reproducability}
It is important to note, that the training process of reinforcement learning and especially multi agent reinforcement learning is hard to evaluate and reproduce. Depending on the initial weights of the neural networks and the layout of the environments, the performance may vary on each restart. Also, the number of workers can 
significantly influence the training performance. To counter the latter, we execute all presented experiments on machines with the same specifications (see \autoref{infrastructure}).\\
Another aspect that is hard to reproduce is training stability. In A3C, an important instrument to prevent the policy from converging too early is using an additional entropy term \cite{a3c}. Our way to maintain stability with changing environments is discussed in \autoref{entropy_balancing_hyperparameter}.
\chapter{Experiment Design and Analysis}\label{chap.experiment}
\section{Reinforcement Learning for Flatland}
\subsection*{A3C Implementation}\label{a3c_implementation}
Originally, the asynchronous advantage actor critic algorithm (A3C) has been designed for use in a single agent environment \cite{a3c}.
By applying it in a multi agent environment, we implicitly convert the environment into a non-stationary environment.
While applying A3C in a multi agent setting, the other agents can be viewed as part of the environment. This means, the behaviour of the environment changes while training, due to the fact that the behaviour of the other agents changes.
Gupta et al.finds in \cite{multiagent_comp_a3c_dqn_etc}, that RL methods like Deep-Q networks (DQN) and Trust region policy optimization (TRPO) are not performing well in a multi agent environment, due to the combination of experience replay and non-stationarity of the environment. We therefore suggest, that it is not recommendable to keep an experience replay buffer with older episodes. Otherwise the sampled experience might represent old agent behaviour which is then learned.
\subsection*{Observation Design}\label{enhanced_observations}
The flatland environment provides a base to build custom observation builders that can be used to create a state representation for the agents as explained in \autoref{observations}. In this work, we do not consider the usage of the grid-based observation builders.
Both the flatland development team as well as S. Huschauer find, that tree based observations work better in their experiments. The flatland specification states \cite{flatland_spec}:
\begin{quote}
Considering, that the actions and directions an agent can chose in any given cell, it becomes clear that a grid like observation around an agent will not contain much useful information, as most of the observed cells are not reachable nor play a significant role in for the agents decisions.
\end{quote}
Based on the provided TreeObsForRailEnv (see \autoref{observations}), we implement a custom observation builder which we use to produce an input vector for our neural network. This observation builder takes the the current state of the environment and produces a fixed size numeric vector with values between 0.0 and 1.0 for each agent. This input vector should fulfil an number of requirements:
\begin{itemize}
	\item Each rail section the agent possibly rides on next should be visible to the agent.
	\item The agent should be able to detect, whether there is another train coming the opposite direction on any section.
	\item The agent should be able to detect on each switch which turn is the faster way to his target.
	\item On switches, the agent should be able to see if a turn does lead to his target, even if it is not the fastest way. If this is the case, taking this turn might even be a good option to evade possibly blocking situations.
	\item For the next grid tile, the agent should be able to detect if it is a switch and if so, if it is one the agent can make a decision on. (see \autoref{flatland_intro} for non-usable switches).
\end{itemize}


\subsection*{Action Space Reduction and Script Policy Actions}\label{reduced_action_space}
The flatland environment is designed in a way to resemble a classical RL environment. This means, on every timestep, we receive observations for each agent, calculate an action and hand this action to the environment, visible in pseudocode \autoref{alg:no_reduction}.\\\\
\begin{algorithm}[H]
	\KwData{initialized flatland environment $\mathcal{E}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents }
	\KwResult{terminal flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
			Fetch action  $\mathcal{u}$ for agent $\mathcal{a}$ at timestep $\mathcal{t}$ based on $\mathcal{s\SPSB{a}{t}}$ from policy $\pi$\\
			$\mathcal{A[a]}$ = $\mathcal{u}$
		}
		
		call $\mathcal{step(A)}$ of $\mathcal{E}$\\
		retrieve reward $\mathcal{R}$\\
		append $\mathcal{A}$ to buffer $\mathcal{B}$\\
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}

	use buffer $\mathcal{B}$ for training of policy $\pi$
	\caption{Default episode for flatland environment}
	\label{alg:no_reduction}
\end{algorithm}
While this makes sense in an environment where agents need to take an action on every timestep (such as Atari games), in flatland most of the time the only reasonable action is to move forward as visible in \autoref{fig:no_desicion}. Only around switches, the actions of an agent have acutual consequences.
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{images/screenshot_no_decision.png}
	\caption{Screenshot from flatland environment. A train heading east. The only reasonable action is to ride forward.}
	\label{fig:no_desicion}
\end{figure}
Every action that is produced by the neural network should be included for training, so the network can adapt to this type of situation. The problem arises now, that all these actions of riding forward are included into the training of the agent. The influence of the actions that actually matter (e.g. the ones around switches) is thereby not as big as it could be, because a large portion of the training data is actually situations that do not actually require decisions.\\
To solve this problem, we implement hard coded rules that the agents follow as long as they are not in a situation to make a decision. Only around switches, the neural network policy is activated. As a consequence, the data used for training has less samples but the samples available are of higher quality. The training with this mechanism implemented looks like \autoref{alg:with_reduction}.
For training, we only use the experience collected near the switch.\\
\begin{algorithm}[H]
	\KwData{initialized flatland environment $\mathcal{env}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents}
	\KwResult{terminal flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			\eIf{agent is near to a switch}{
				get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
				%$\tcp{Fetch action for agent, based on current state of agent}$
				$\mathcal{A[a]} \leftarrow from policy \pi (\mathcal{s\SPSB{a}{t}})$\\
			}{
				$\mathcal{A[a]} \leftarrow \mathcal{u_{forward})}$
			}
		}
		call $\mathcal{env.step(A)}$ \\
		retrieve reward $\mathcal{R}$\\
		\If{agent was near switch}{
			append $\mathcal{s_{t}}$, $\mathcal{A}$ and $\mathcal{R}$ to buffer $\mathcal{B}$\\
		}
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}
	use buffer $\mathcal{B}$ for training of policy $\pi$

	\caption{Improved learning for flatland environment}
	\label{alg:with_reduction}
\end{algorithm}
This drastically improves training performance as visible in \autoref{chart:action_reduction_comparison}
\begin{figure}[H]
	\begin{center}
		\input{diagrams/comparison_action_reduction.pgf}
	\end{center}
	\caption{Comparison of training with and without action reduction}
	\label{chart:action_reduction_comparison}
\end{figure}

\subsection*{Neural Network Architecture}\label{network_architecture}
In RL, the architecture of neural networks is often very primitive. The reason for this is, that 


\subsection*{Curriculum Learning and Reward assignment}\label{curriculum_learning}
The reward assignment in flatland can be freely configured. But as long as there is not some distance-to-target dependent reward function, the probability, that an agent with an uninformed policy finds its target is small. This is especially the case for large environments with many trains on it. For example, most evaluation environments of flatland round 2 have up to 200 individual agents and are up 100x100 tiles large (SOURCE!). The rollout of such an environment takes a lot more time than the rollout of a 20x20 environment with 5 trains. Also, the probability, that a train arrives in a small environment is larger and therefore, the experience is more valuable for training.
To improve training times, it makes therefore sense to start with a small environment and move to larget ones, once the agents mastered pathfinding and basic collision avoidance.

\subsection*{Entropy Balancing and Hyperparameter Tuning}\label{entropy_balancing_hyperparameter}
In RL, it is of great importance to find the right combination of exploration and explotation. During exploration, the agent explores as much of the state space as possible. This enables the agent to later exploit the found states which are beneficial. Without this exploration, there is a chance that the agent settles on suboptimal policies too quickly and ignores parts of the state space the agent has never seen.\\
The policy is characterized by a probability distribution of actions.

\subsection*{Agent Communication}\label{agent_communication}
We asked ourselves how blocking is avoided in the real world?\\
On the SBB rail environment traffic lights are used to prevent collisions between trains. The traffic lights are controlled by a central control point.\\
On the streets signs, traffic lights and basic rules like right before left are used to prevent collisions. \\
If signs and traffic lights are missing for example high up in the mountains, buses are using their horn to inform upcoming vehicles. \\
Our idea was to implement such a communication between the agents.
We consider 3 different options of communication:
\begin{itemize}
	\item Agents can choose what they communicate
	\item Agents communicate their next action
	\item Implicit communication
\end{itemize}

We start with open communication, which means the agent can decide by himself what he is going to communicate to the other agents... %TODO: continue

Communication in multi agent RL is a topic of active research and considered very complex. As we could not reach the same performance with communication as without using communication, we deciced to remove the explicit communication.


\section{Distributed Architecture and Parallelism}\label{dist_architecture}
\subsection*{Infrastructure}\label{infrastructure}
We use various computers and servers to train our model. Most of the time we run the training of our model on a test infrastructure server of the ZHAW School of Engineering.
The server consists out of 56 CPU cores and 721Gb memory. We connect 3 Openstack machines with 8 CPU cores each to this server, to increase our training. \\
The flatland environment is running on each cpu core and the results are sent to a web server which updates the model and sent the updated neuronal network weights back to the client. \\
The reason why we are using CPU cores over GPU performance is that the reinforcement algorithm A3C performs better on CPUs instead of GPUs. \\  %TODO: Grund eventuell erweitern?

\begin{gather*}
\includegraphics[width=400pt]{images/visio/architecture.png}
\end{gather*}

All experiments (mentioned in \autoref{enhanced_observations}) are made on a Openstack Machine with 8 CPU cores and 64Gb memory.
Each experiment ran for 12 hours.\\

We ran an experiment to compare the learning progress between a single worker and 7 workers.\\
The experiment shows that the learning progress with 7 workers is over 2.5 times faster than the learning progress with a single worker.\\
It also shows that the learning curve fluctuates more with a higher amount of workers.\\
%TODO: Update plot with 55 workers and compare it.
%TODO: Fix problem with misplaced image.

\begin{figure}
	\begin{center}
		\input{diagrams/singlemany_comparison.pgf}
	\end{center}
	\caption{Comparison of training with 1 worker and with 7 workers}
	\label{chart:singlemany_comparison}
\end{figure}

\subsection*{Distributed training}
We decided during round 1 of the Flatland challenge to change the training of the neuronal network to a distributed approach. \\
We use the computer with the most memory and cores to run a web server, which distributes the neuronal network, the observation builder and also certain input parameters at training start. \\
The workers, all other CPUs on which an instance of the Flatland environment is running, receive the files and compile them into C code with Cython.\\
After the training start the general cycle for each worker is as following:

\begin{enumerate}
	\item Get current model from web server
	\item Execute episode on Flatland environment
	\item Calculate gradient and upload weights
	\item Repeat
\end{enumerate}





\begin{itemize}
\item (Beschreibt die Grund√ºberlegungen der realisierten L√∂sung (Konstruktion/Entwurf) und die Realisierung als Simulation, als Prototyp oder als Software-Komponente)
\item (Definiert Messgr√∂ssen, beschreibt Mess- oder Versuchsaufbau, beschreibt und dokumentiert Durchf√ºhrung der Messungen/Versuche)
\item (Experimente)
\item (L√∂sungsweg)
\item (Modell)
\item (Tests und Validierung)
\item (Theoretische Herleitung der L√∂sung)
\end{itemize}

\section{Technical Implementation Aspects}\label{software}
We used the following tools in our project.

\subsection*{Working Environment}\label{os}
\begin{itemize}
	\item Microsoft Windows 10
	\item Ubuntu 19.04
\end{itemize}

\subsection*{Visual Studio Code}\label{vsc}
\begin{itemize}
	\item Visual Studio Code 1.40
\end{itemize}

\subsection*{Documentation}\label{dokutools}
\begin{itemize}
	\item XeLateX with Visual Studio Code
	\item XeLateX with WebStorm
\end{itemize}


\subsection*{Programming language}\label{programminglanguages}
\begin{itemize}
	\item Python 3.6
\end{itemize}

\subsection*{Python modules}\label{modules}
\begin{itemize}
	\item Flatland-rl 1.3 - 2.1.10
	\item Tensorboard 2.0
	\item Keras x.x
	\item Cython x.x
	\item %TODO: finish this
\end{itemize}


\section{Basic considerations}
%split into round 1 / round 2

\subsection{Round 1}
%Observations
%Convolutional network + Global observations
%Early stopping
%Reward vergabe angepasst
We started with rebuilding the A3C algorithm from S. Huschauer to get a better knowledge how A3C works.\\
We made some experiments with different observations: TreeObservations and GlobalOberservations.
Because we made better and faster progress with GlobalObservatione we continued with those and combind them with a convolutional network.
Right after starting this project, we faced a problem regarding the reward distribution.\\


\subsection{Round 2}




\section{Measurands}
%Messgr√∂ssen: Evaluator, benchmark

\section{Experiments}
%

\section{Solution approach}
%Neue Actions


\section{Testing and submissions}

\section{Theoretical derivation of the solution}



