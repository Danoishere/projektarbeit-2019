
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage f√ºr Windows ProTeXt mit TexMakerX
% Title       : 
% File        : vorgehen.tex Rev. 00
% Date        : 7.5.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Approach and Methodology}\label{chap.vorgehen}
\section{Iterative Work Approach}\label{basic_cons}
As described under \autoref{baseline}, our work is based on the work of S. Huschauer \cite{flatlandstephan}. We take the idea of using the A3C algorithm to solve the Flatland problem and try various modifications in an attempt to improve its performance. We proceed by giving an idea, what we want to achieve by changing the specified part, followed by an experiment to either prove or disprove our hypothesis. We do this in an iterative manner, to gradually come closer to a well-performing solution for Flatland.\\
In our experiments, we focus exclusively on Flatland round 2. While we compare our solution for round 1 in \autoref{chap.resultate}, all technical improvements are also part of our solution for round 2.\\
Our work can be categorized into a section about reinforcement learning for Flatland (\autoref{rl_flatland}) and a section about infrastructure and scaling up the training process (\autoref{dist_architecture}).\\
\section[Reproducibility]{Reproducibility in Reinforcement Learning}\label{reproducability}
It is important to note, that the training process of reinforcement learning and especially multi agent reinforcement learning can be hard to reproduce. Depending on the initial weights of the neural networks and the layout of the environments, the performance may vary on each restart. Also in a distributed algorithm, the number of workers can significantly influence the training performance. If not differently specified, we execute all presented experiments on machines with the same specifications (see \autoref{infrastructure}).\\
Another aspect that is hard to reproduce is training stability. In A3C, an important instrument to prevent the policy from converging too early is using an additional entropy term \cite{a3c}. Our way to maintain stability with changing environments is discussed in \autoref{entropy_balancing_hyperparameter}.\\
For better comparability, we try to keep our experimental versions of the training as similar to each other as possible. If not differently specified, we run evaluations with the following setup:
\begin{itemize}
	\item We use an environment of the size 100x100 tiles with 14 individual agents.
	\item The map contains 20 cities
	\item There is a maximum of 3 rails between cities and a maximum of 4 rails inside cities
	\item Based on the flatland specification, the maximum allowed number of timesteps is 1608 \cite{flatland_spec}.
\end{itemize}
To analyze the performance of the solution, we run an analyzer that executes 20 rollouts of the environment using the same neural network parameters. After the these 20 rollouts, we update the neural network parameters. For each evaluation round, we use the same 20 environment layouts. 
To compare the performance in a graph, we now take the mean number of agents arrived at their target and plot that in our graph.
\chapter[Experiments]{Experiment Design and Analysis}\label{chap.experiment}
\section[RL for Flatland]{Reinforcement Learning for Flatland}\label{rl_flatland}
\subsection*{A3C Implementation}\label{a3c_implementation}
Originally, the asynchronous advantage actor critic algorithm (A3C) has been designed for use in a single agent environment \cite{a3c}.
By applying it in a multi agent environment, we implicitly convert the environment into a non-stationary system.
While applying A3C in a multi agent setting, the other agents can be viewed as part of the environment. This means, the behaviour of the environment changes while training, due to the fact that the behaviour of the other agents changes.
Gupta et al.finds in \cite{multiagent_comp_a3c_dqn_etc}, that RL methods like Deep-Q networks (DQN) and Trust region policy optimization (TRPO) are not performing well in a multi agent environment, due to the combination of experience replay and non-stationarity of the environment. We therefore suggest, that it is not recommendable to keep an experience replay buffer with older episodes. Otherwise the sampled experience might represent old agent behaviour which is then learned to deal with.\\
An important implementation detail in our version is, that we do not perform updates during episodes but only at their end. The reason for this is, that the only possibility for an agent to receive reward is at the end of the episode. Therefore would any earlier update have a reward of 0 and not help the training process.
\subsection*{Observation Design}\label{enhanced_observations}
The Flatland environment provides a base to build custom observation builders that can be used to create a state representation for the agents as explained in \autoref{observations}. In this work, we do not consider the usage of the grid-based observation builders.
Both the Flatland development team as well as S. Huschauer find, that tree based observations work better in their experiments \cite{flatlandstephan}. The Flatland specification states \cite{flatland_spec}:
\begin{quote}
Considering, that the actions and directions an agent can chose in any given cell, it becomes clear that a grid like observation around an agent will not contain much useful information, as most of the observed cells are not reachable nor play a significant role in for the agents decisions.
\end{quote}
Based on the provided TreeObsForRailEnv (see \autoref{observations}), we implement a custom observation builder which we use to produce an input vector for our neural network. This observation builder takes the the current state of the environment and produces a fixed size numeric vector with values between 0.0 and 1.0 for each agent. This input vector should fulfil an number of requirements:
\begin{itemize}
	\item Each rail section the agent possibly rides on next should be visible to the agent.
	\item The agent should be able to detect, whether there is another train coming the opposite direction on any section.
	\item The agent should be able to detect on each switch which turn is the faster way to his target.
	\item On switches, the agent should be able to see if a turn does lead to his target, even if it is not the fastest way. If this is the case, taking this turn might even be a good option to evade possibly blocking situations.
	\item For the next grid tile, the agent should be able to detect if it is a switch and if so, if it is one the agent can make a decision on. (see \autoref{flatland_intro} for non-usable switches).
\end{itemize}
The provided TreeObsForRailEnv produce a graph with a node for each section of the rail. We extend these nodes with additional information about train traffic coming the other direction than the one the agent is heading. We take the information from these nodes and convert them into a numeric vector. In case that there is a dead end, we fill the observation with zeros (this could only happen in Flatland round 1, round 2 does not have dead ends). After the conversion, we concatenate all section observations into one large vector with information for all upcoming sections.\\
While this vector already contains all required information outside of the agent, we add another vector with information regarding the agent itself (train specific observation in \autoref{obs_descr}). This vector contains the speed of the agent, the max. speed of the agent, the type of the current tile, the direction the agent is heading and 
\begin{figure}
	\centering
	\includegraphics[width=300pt]{diagrams/tree_obs_mapping.pdf}
	\caption{Illustration of the tree observation mapped onto the Flatland enviornment. Each colored bar represents information about a section.}
	\label{obs_descr}
\end{figure}
\subsection*{Action Space Reduction and Script Policy Actions}\label{reduced_action_space}
The Flatland environment is designed in a way to resemble a classical RL environment. This means, on every timestep, we receive observations for each agent, calculate an action and hand this action to the environment, visible in pseudocode \autoref{alg:no_reduction}.\\\\
\begin{algorithm}[H]
	\KwData{initialized Flatland environment $\mathcal{E}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents }
	\KwResult{terminal Flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
			// Fetch action for agent, based on current state\\
			$\mathcal{A[a]} \leftarrow$ from policy $\pi (\mathcal{s\SPSB{a}{t}})$\\
		}
		
		call $\mathcal{env.step(A)}$ \\
		retrieve reward $\mathcal{R}$\\
		append $\mathcal{A}$ to buffer $\mathcal{B}$\\
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}

	use buffer $\mathcal{B}$ for training of policy $\pi$
	\caption{Default episode for Flatland environment}
	\label{alg:no_reduction}
\end{algorithm}
While this makes sense in an environment where agents need to take an action on every timestep (such as Atari games), in Flatland most of the time the only reasonable action is to move forward as visible in \autoref{fig:no_desicion}. Only around switches, an agent needs to take actions other than just keep going forward.
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{images/screenshot_no_decision.png}
	\caption{Screenshot from Flatland environment. A train heading east. The only reasonable action is to ride forward.}
	\label{fig:no_desicion}
\end{figure}
Every action that is produced by the neural network should be included for training, so the network can adapt to this type of situation. The problem arises now, that all these actions of riding forward are included into the training of the agent. The influence of the actions that actually matter (e.g. the ones around switches) is thereby not as big as it could be, because a large portion of the training data is actually situations that do not actually require decisions.\\
To solve this problem, we implement hard coded rules that the agents follow as long as they are not in a situation to make a decision. Only around switches, the neural network policy is activated. As a consequence, the data used for training has less samples but the samples available are of higher quality. The training with this mechanism implemented looks like \autoref{alg:with_reduction}.
For training, we only use the experience collected near the switch.\\
\begin{algorithm}[H]
	\KwData{initialized Flatland environment $\mathcal{env}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents}
	\KwResult{terminal Flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			\eIf{agent is near to a switch}{
				get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
				// Fetch action for agent, based on current state\\
				$\mathcal{A[a]} \leftarrow$ from policy $\pi (\mathcal{s\SPSB{a}{t}})$\\
			}{
				$\mathcal{A[a]} \leftarrow \mathcal{u_{forward})}$
			}
		}
		call $\mathcal{env.step(A)}$ \\
		retrieve reward $\mathcal{R}$\\
		\If{agent was near switch}{
			append $\mathcal{s_{t}}$, $\mathcal{A}$ and $\mathcal{R}$ to buffer $\mathcal{B}$\\
		}
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}
	use buffer $\mathcal{B}$ for training of policy $\pi$

	\caption{Improved learning for flatland environment}
	\label{alg:with_reduction}
\end{algorithm}
This drastically improves training performance as visible in \autoref{chart:action_reduction_comparison}. While both versions have the potential to perform well, based on the observation data, we observe that the version without the action reduction is not able to performe nearly as well as the improved version with reduced actions.
\begin{figure}[H]
	\begin{center}
		\input{diagrams/comparison_action_reduction.pgf}
	\end{center}
	\caption{Comparison of training with and without action reduction}
	\label{chart:action_reduction_comparison}
\end{figure}
As a consequence, we keep the reduced action mechanism as part of our final solution.
\subsection*{Neural Network Architecture}\label{network_architecture}
In RL, the architecture of neural networks is often rather simple \cite{mnih2013playing}\cite{a3c}. The already sample inefficient process of reinforcement learning should not be slowed down by a difficult architecture. Differently than S. Huschauer in \cite{flatlandstephan}, we do not use a convolutional neural network. The reason for this is the way our observation vector is composed (see \autoref{enhanced_observations}). In this vector, every element corresponds with an actual information in the mapped rail section. Convolutional layers would especially make sense if there were patterns to extract from the observation vector.\\
Another more relevant aspect of neural networks is the topic of recurrent layers. Recurrent layers allow the agent to remember information from previous timesteps. The original A3C publication shows, that it is possible to achieve a significant performance improvement using long short-term memory layers (LSTM) \cite{a3c}. Also S. Huschauer uses an LSTM-block to improve training. To quantify this improvement, we run an experiment in \autoref{chart:lstm_comparison} to compare a version with recurrent layer to a version without.

\begin{figure}[H]
	\begin{center}
		\input{diagrams/lstm_comparison.pgf}
	\end{center}
	\caption{Comparison of training performance between a version with an LSTM-layer and a version without. It is evident that the version with LSTM performs better than the version without.}
	\label{chart:lstm_comparison}
\end{figure}

It is clearly visible that an LSTM-layer helps the training process and we therefore keep the LSTM-layer as part of our solution.

\subsection*{Curriculum Learning and Reward assignment}\label{curriculum_learning}
The reward assignment in Flatland can be freely configured. But as long as there is not some distance-to-target dependent reward function, the probability, that an agent with an uninformed policy finds its target is small. This is especially the case for large environments with many trains on it. For example, most evaluation environments of Flatland round 2 have up to 200 individual agents and are up 100x100 tiles large (SOURCE!). The rollout of such an environment takes a lot more time than the rollout of a 20x20 environment with 5 trains. Also, the probability, that a train arrives in a small environment is larger and therefore, the experience is more valuable for training.
To improve training times, it makes therefore sense to start with a small environment and move to larget ones, once the agents mastered pathfinding and basic collision avoidance.
In order to verify the meaningfulness of such a curriculum, we run an experiment with a large environment with dimensions 100x100 and 50 individual agents. One experiment tries to learn its policy directly using this environment. The other experiment uses a curriculum that gradually gets more difficult.
As visible in \autoref{chart:curriculum_comparison}, the version without curriculum is not able to learn from the experience. We suspect that is is caused by too much variance in the environment and not enough successful experience to learn from. After all, an agent needs to reach its target to get reward, and in a large environment combined with an uninformed policy, this is potentially very difficult.
\begin{figure}[H]
	\begin{center}
		\input{diagrams/curriculum_comparison.pgf}
	\end{center}
	\caption{Comparison of training performance between a version with and a version without curriculum learning. It appears, that the version without curriculum is not able to generalize well in larger environments.}
	\label{chart:curriculum_comparison}
\end{figure}
\begin{table}
	\centering
	\begin{tabular}{ |l|c|c|c|c|c| } 
		\hline
		& \textbf{Level 0} 
		& \textbf{Level 1}  
		& \textbf{Level 2} 
		& \textbf{Level 3} 
		& \textbf{Level 4}\\
		\hline
		\textbf{Next level on successrate} & 70\% & 70\% & 75\% & 70\% & 60\% \\  
		\textbf{Nr. of agent} & 4 & 8 & 12 & 16 & 20 \\  
		\textbf{Env. size} & 25x25 & 30x30  & 40x40 & 50x50 & 50x50 \\ 
		\textbf{Num. cities} & 5 & 8 & 10  & 12 & 16 \\ 
		\textbf{Max. rails between cities} & 1 & 2 & 2 & 2 & 2  \\ 
		\textbf{Max. rails in city} & 2 & 2 & 3 & 3 & 3  \\ 
		\hline
	\end{tabular}
	\label{table:curriculum_data}
	\caption{Curriculum level specifications for our experiment to compare a version with curriculum to a version without.}
\end{table}
While it is clearly visible in \autoref{chart:curriculum_comparison} that directly training on difficult levels is not feasible, it also appears that the learning speed is slowed down on higher curriculum levels. With increasingly more difficult levels and more agents, the frequency of available network updates is being slowed down due to the fact that a rollout of the environment takes longer. We therefore think, it is desirable to keep the curriculum levels small and primarily increase the number of agents instead of the size of the environment.
\subsection*{Entropy Balancing and Hyperparameter Tuning}\label{entropy_balancing_hyperparameter}
In RL, it is of great importance to find the right combination of exploration and explotation \cite{explorationexploitation}. During exploration, the agent explores as much of the state space as possible. This enables the agent to later exploit the found states which are beneficial. Without this exploration phase, there is a chance that the agent settles on suboptimal policies too quickly and ignores parts of the state space the agent has never seen and therefore does not consider.\\
The policy is characterized by a probability distribution of actions.

\subsection*{Agent Communication}\label{agent_communication}
The topic if of communication in multi agent RL is a topic of ongoing research. As a starting point for this work we asked ourselves how blocking is avoided in the real world. On the SBB rail environment railroad signals are used to prevent collisions between trains. These signals are controlled by a central control instance that directs the traffic of all trains on the grid.
The Flatland environment does not provide a communication channel nor a rail road signal system, but participants of the challenge are allowed to implement one themselfes. In the following discussion, we summarize these options under the topic of communication and centralized planning. An initial intuition for the problem gives the question, if it was a human guiding the train, would she/he be able to successfully steer the train to its assigend target without communication?
We assume that this is not the case. While classical traffic rules (like right has priority over left) can help to avoid collisions, we do think that in the case of railroad traffic this does not suffice. In the real world, rail traffic is controlled by a control center that gives the trains permission to go or tells them to wait. Having one central control instances solves one of the big problems of such a system which is, that there is a chance that two agents take an action at the same moment which does bring both of them into a non resolvable situation. An example for such a situation would be two agents, that enter a rail section at the same timestep. Both of them would have had the possibility to choose a different path, but both perceived the section as empty and decided to enter. Combined with the fact that trains cannot drive backwards, both trains will not be able to arrive at their assigned targets. As a solution to this dilemma, we discuss three solutions that could work to resolve such situations.
\begin{itemize}
	\item \textbf{Negotiation:} To solve the problem of conflicting actions at the same timestep, it would be possible to introduce a iterative communication channel, on which the agents can negotiate, what agent is allowed to take an action next and who has to wait another timestep. The number of iterations could be determined by the outcome of the process. As long as there is no resolution about who is allowed to take an action and who is not, another negotiation round is added.\\
	While this procedure might help to avoid blocking situations, it could certainly not completely remove them. Especially complex situations with many agents could still prove to be difficult, even with a negotiation mechanism in place.

	\item \textbf{Prioritized planning:} An approach to solve the dilemma of conflicting actions at the same timestep could be to introduce an artifical order of importance among the agents. Then the next $\mathcal{n}$ timesteps could be planned for the most important agent. The second agent now takes the planning of the first agent into account and tries to come up with a policy that does not obstruct the planned route of the first agent. This process is repeated for all following agents.\\
	We think that possible conflicts could maybe be resolved by backtracking and adjusting the priorities of the trains.	
	
	\item \textbf{Unconstrained communication:} While negotiation would prevent an agent from taking an action, the goal of unconstrained actions would not be to constrain the action space but rather to convince the agent to choose an action \textit{stop} or \textit{do nothing} and wait for the next timestep if the situation is uncertain. Also for unconstrainted actions, it would make sense to be an iterative process, similar to the negotiation approach. As long as not all agents mark their communication as completed, another communication round is added.\\
	We suspect that a problem with this approach could be that it would have much slower convergence than the negotiation approach. The reason for this assumption is the fact, that such an unconstrained communication would not directly influcence the actions of the agents. It would be nescessary to learn both the "speaking" and the "interpretation", without a direct consequences.
\end{itemize}

\section[Distributed Training]{Distributed Architecture and Parallelism}\label{dist_architecture}
\subsection*{Distributed training}
One of the main advantages of the A3C algorithm is its ability to be used in a distributed manner. This allows to run multiple versions of the environment asynchronously and collect the updates for both the actor and the critic network in a central place. To fully take advantage of this mechanism, we implemented a system that allows to run a large number of environements at the same time. All running training instances contribute to the same central network. Thanks to HTTP-based connections, we can use this mechanism even between computers or even networks. This approach is only limited by the capability of the central model server and the network throughput.\\
Additionally, our central model server not only handles the update of the neural network but also distributes both the code for building observations and a file with all hyperparameter required for training on startup. The startup process of the worker node then converts the code for building observations into native C-code using Cython. This code converted code then gets dynamically imported.\\
Using native C-code speeds up the rollout of the environment and therefore the training process by a factor of 2 to 5.
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{diagrams/distributed_training_update.pdf}
	\caption{Simplified procedure of training of a single worker process. The communication between the worker and the model server works by using HTTP. The compilation of the observation.pyx file is only done once on each machine.}
	\label{obs_descr}
\end{figure}
To improve the performance of the system, we compress both the weights (server to worker) and the gradient update (worker to server) using ZLIB. This reduces the size of the transmitted data by (COMPLETE WITH PERCENTAGE).
\begin{figure}[H]
	\centering
	\includegraphics[width=400pt]{images/visio/architecture.png}
	\caption{Illustration of distributed architectures with main node (node 1) and several additional nodes (2-w) connected over HTTP.}
	\label{dist_architecture_img}
\end{figure}
To analyze the impact of training a policy with multiple workers a the same time, we run an experiment that compares having a single worker to training with 7 workers at the same time.
While it is appearant in \autoref{chart:singlemany_comparison} that having mulitple workers does speed up the training process, we do not observe any impact of the number of workers on the quality of the policy.
\begin{figure}
	\begin{center}
		\input{diagrams/singlemany_comparison.pgf}
	\end{center}
	\caption{Comparison of training with 1 worker and with 7 workers}
	\label{chart:singlemany_comparison}
\end{figure}
\subsection*{Infrastructure}\label{infrastructure}
The infrastructure used for this project consists of 4 machines used in various combinations.
\begin{itemize}
	\item \textbf{Draft Animal (DRAN):} Server with 56 CPUS and 721 GB of RAM. This machine is mainly used to train the current submission model.
	\item \textbf{Openstack machines:} 3 servers with 8 CPUS each. One machine has 16 GB of RAM, the other two have 64 GB each.
\end{itemize}

The Flatland environment is running on each cpu core and the results are sent to a web server which updates the model and sent the updated neural network weights back to the client. \\
The reason why we are using CPU cores over GPU performance is that the reinforcement algorithm A3C performs better on CPUs instead of GPUs. \\  %TODO: Grund eventuell erweitern?

All experiments (mentioned in \autoref{enhanced_observations}) are made on a Openstack Machine with 8 CPU cores and 64Gb memory.
Each experiment ran for 12 hours.\\

We ran an experiment to compare the learning progress between a single worker and 7 workers.\\
The experiment shows that the learning progress with 7 workers is over 2.5 times faster than the learning progress with a single worker.\\
It also shows that the learning curve fluctuates more with a higher amount of workers.\\
%TODO: Update plot with 55 workers and compare it.
%TODO: Fix problem with misplaced image.



\begin{itemize}
\item (Beschreibt die Grund√ºberlegungen der realisierten L√∂sung (Konstruktion/Entwurf) und die Realisierung als Simulation, als Prototyp oder als Software-Komponente)
\item (Definiert Messgr√∂ssen, beschreibt Mess- oder Versuchsaufbau, beschreibt und dokumentiert Durchf√ºhrung der Messungen/Versuche)
\item (Experimente)
\item (L√∂sungsweg)
\item (Modell)
\item (Tests und Validierung)
\item (Theoretische Herleitung der L√∂sung)
\end{itemize}

\section{Technical Implementation Aspects}\label{software}
We used the following tools in our project.

\subsection*{Working Environment}\label{os}
\begin{itemize}
	\item Microsoft Windows 10
	\item Ubuntu 19.04
\end{itemize}

\subsection*{Visual Studio Code}\label{vsc}
\begin{itemize}
	\item Visual Studio Code 1.40
\end{itemize}

\subsection*{Documentation}\label{dokutools}
\begin{itemize}
	\item XeLateX with Visual Studio Code
	\item XeLateX with WebStorm
\end{itemize}


\subsection*{Programming language}\label{programminglanguages}
\begin{itemize}
	\item Python 3.6
\end{itemize}

\subsection*{Python modules}\label{modules}
\begin{itemize}
	\item Flatland-rl 1.3 - 2.1.10
	\item Tensorboard 2.0
	\item Keras x.x
	\item Cython x.x
	\item %TODO: finish this
\end{itemize}


