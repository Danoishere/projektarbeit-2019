%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage für Windows ProTeXt mit TexMakerX
% Title       : 
% File        : vorgehen.tex Rev. 00
% Date        : 7.5.12
% Author      : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Approach and methodology}\label{chap.vorgehen}
\section{Basic considerations}\label{basic_cons}
As described under (tODO: ref to first mention), our work is based on the work of S. Huschauer (REF). We take his idea of using the A3C algorithm to solve the flatland problem and try various modifications in an attempt to improve its performance. \\We proceed by giving an intution, what we want to achieve by changing the specified part, followed by an experiment to either prove or disprove our hypothesis.\\
For training purposes, we started by reimplementing the algorithm by ourselfes. This enabled us from the beginning to gain a deeper understanding of how the algorithm works and where we could find possible areas for improvement. From there, we iteratively added these potential improvements to later compare them against the original version.
In this work, we proceed by comparing the final version to versions without these features. 
It is important to note, that the training process of reinforcement learning and especially multi agent reinforcement learning is hard to evaluate. Depending on the initial weights of the neural networks and the shape of the environments, the performance may vary on each restart. Also, the number of workers can 


\section{A3C implementation for flatland}\label{enhanced_observations}
Originally, the asynchronous advantage actor critic algorithm (A3C) has been designed for use in a single agent environment.
By applying it in a multi agent environment, we implicitly convert the environment into a non-stationary environment.
While applying A3C in a multi agent setting, the other agents can be viewed as part of the environment. This means, the behaviour of the environment changes while training, due to the fact that the behaviour of the other agents changes. 

Gupta et al. \cite{multiagent_comp_a3c_dqn_etc} finds, that methods like Deep-Q networks (DQN) and Trust region policy optimization (TRPO) are not performing well in a multi agent environment, due to the combination of experience replay and non-stationarity of the environment. We therefore suggest, that it is not recommendable to keep an experience replay buffer with older episodes. Otherwise the sampled experience might represent old agent behaviour which is then learned.

\section{Enhanced observations}\label{enhanced_observations}
Based on the provided observation of the flatland environment (see  \autoref{observations}), we implement a updated version of the tree observation in form of a binary tree. 

\section{Distrubuted architecture and parallelism}\label{dist_architecture}
Für die vorliegende Arbeit wurden die unten aufgeführten Programme eingesetzt.

\section{Action space reduction}\label{reduced_action_space}
Für die vorliegende Arbeit wurden die unten aufgeführten Programme eingesetzt.

\section{Curriculum learning}\label{curriculum_learning}


\section{Entropy balancing}\label{enhanced_observations}
In RL, it is of great importance to find the right combination of exploration and explotation. During exploration, the agent explores as much of the state space as possible. This enables the agent to later exploit the found states which are beneficial. Without this exploration, there is a chance that the agent settles on suboptimal policies too quickly and ignores parts of the state space the agent has never seen.\\
The policy is characterized by a probability distribution of actions.

\section{Agent communication}\label{reduced_action_space}
Für die vorliegende Arbeit wurden die unten aufgeführten Programme eingesetzt.





\begin{itemize}
\item (Beschreibt die Grundüberlegungen der realisierten Lösung (Konstruktion/Entwurf) und die Realisierung als Simulation, als Prototyp oder als Software-Komponente)
\item (Definiert Messgrössen, beschreibt Mess- oder Versuchsaufbau, beschreibt und dokumentiert Durchführung der Messungen/Versuche)
\item (Experimente)
\item (Lösungsweg)
\item (Modell)
\item (Tests und Validierung)
\item (Theoretische Herleitung der Lösung)
\end{itemize}

\section{(Used Software)}\label{software}
We used the following tools in our project.

\subsection*{Working Environment}\label{os}
\begin{itemize}
	\item Microsoft Windows 10
	\item Ubuntu 19.04
\end{itemize}

\subsection*{Visual Studio Code}\label{vsc}
\begin{itemize}
	\item Visual Studio Code 1.40
\end{itemize}

\subsection*{Documentation}\label{dokutools}
\begin{itemize}
	\item XeLateX with Visual Studio Code
	\item XeLateX with WebStorm
\end{itemize}


\subsection*{Programming language}\label{programminglanguages}
\begin{itemize}
	\item Python 3.6
\end{itemize}

\subsection*{Python modules}\label{modules}
\begin{itemize}
	\item Flatland-rl 1.3 - 2.1.10
	\item Tensorboard 2.0
	\item Keras x.x
	\item Cython x.x
	\item %TODO: finish this
\end{itemize}


\section{Basic considerations}
%split into round 1 / round 2

\subsection{Round 1}
%Observations
%Convolutional network + Global observations
%Early stopping
%Reward vergabe angepasst
We started with rebuilding the A3C algorithm from S. Huschauer to get a better knowledge how A3C works.\\
We made some experiments with different observations: TreeObservations and GlobalOberservations.
Because we made better and faster progress with GlobalObservatione we continued with those and combind them with a convolutional network.
Right after starting this project, we faced a problem regarding the reward distribution.\\


\subsection{Round 2}




\section{Measurands}
%Messgrössen: Evaluator, benchmark

\section{Experiments}
%

\section{Solution approach}
%Neue Actions


\section{Testing and submissions}

\section{Theoretical derivation of the solution}



