\usepackage{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  _____   ____  _____                                          %
% |_   _| /  __||  __ \    Institute of Computitional Physics   %
%   | |  |  /   | |__) |   Zuercher Hochschule Winterthur       %
%   | |  | (    |  ___/    (University of Applied Sciences)     %
%  _| |_ |  \__ | |        8401 Winterthur, Switzerland         %
% |_____| \____||_|                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Project     : LaTeX doc Vorlage f√ºr Windows ProTeXt mit TexMakerX
% Title       : 
% File        : vorgehen.tex Rev. 00
% Date        : 7.5.12
%       : Remo Ritzmann
% Feedback bitte an Email: remo.ritzmann@pfunzle.ch
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Basic Implementation}\label{chap.basic_implementation}
\section{A3C Implementation}\label{a3c_implementation}
Originally, the asynchronous advantage actor critic algorithm (A3C) has been designed for use in a single agent environment \cite{a3c}.
By applying it in a multi agent environment, we implicitly convert the environment into a non-stationary system.
While applying A3C in a multi agent setting, the other agents can be viewed as part of the environment. This means, the behaviour of the environment changes while training, due to the fact that the behaviour of the other agents changes.
Gupta et al.finds in \cite{multiagent_comp_a3c_dqn_etc}, that RL methods like Deep Q-networks (DQN) and Trust region policy optimization (TRPO) are not performing well in a multi agent environment, due to the combination of experience replay and non-stationarity of the environment. We therefore suggest, that it is not recommendable to keep an experience replay buffer with older episodes. Otherwise the sampled experience might represent old agent behaviour which is then learned to deal with.\\
An important implementation detail in our version is, that we do not perform updates during episodes but only at their end. The reason for this is, that the only possibility for an agent to receive reward is at the end of the episode. Therefore would any earlier update have a reward of 0 and not help the training process.

\section{Entropy Balancing}\label{entropy_balancing_hyperparameter}
In RL, it is of great importance to find the right combination of exploration and explotation \cite{explorationexploitation}. During exploration, the agent explores as much of the state space as possible. This enables the agent to later exploit the found states which are beneficial. Without this exploration phase, there is a chance that the agent settles on suboptimal policies too quickly and ignores parts of the state space the agent has never seen and therefore does not consider in his action selection. To avoid an early convergence in A3C, it is common to use an additional entropy term. This entropy term is defined as \cite{a3c}:
\begin{gather*}
H(p) = -\sum_{u=0}^{U}log(p_{u})*p_{u}
\end{gather*}
Where $\mathcal{p}$ is a possibility distribution over all available actions $\mathcal{U}$. The entropy is multiplied with a factor $\beta$ and added to the policy loss. This has an effect of preventing a convergence towards a single action, especially in the early phase of training. Without this entropy term, we often observe such an early convergence with grave consequences for the training perfromance. We also observe, that it is recommendable to reduce this factor $\beta$ to zero, once a stage of training is achieved, where it is not nescessary to explore anymore.
\section{Observation Design}\label{enhanced_observations}
The Flatland environment provides a base to build custom observation builders that can be used to create a state representation for the agents as explained in \autoref{observations}. In this work, we do not consider the usage of the grid-based observation builders.
Both the Flatland development team as well as S. Huschauer find, that tree based observations work better in their experiments \cite{flatlandstephan}. The Flatland specification states \cite{flatland_spec}:
\begin{quote}
	Considering, that the actions and directions an agent can chose in any given cell, it becomes clear that a grid like observation around an agent will not contain much useful information, as most of the observed cells are not reachable nor play a significant role in for the agents decisions.
\end{quote}
Based on the provided TreeObsForRailEnv (see \autoref{observations}), we implement a custom observation builder which we use to produce an input vector for our neural network. This observation builder takes the the current state of the environment and produces a fixed size numeric vector with values between 0.0 and 1.0 for each agent. This input vector should fulfil an number of requirements:
\begin{itemize}
	\item Each rail section the agent possibly rides on next should be visible to the agent.
	\item The agent should be able to detect, whether there is another train coming the opposite direction on any section.
	\item The agent should be able to detect on each switch which turn is the faster way to his target.
	\item On switches, the agent should be able to see if a turn does lead to his target, even if it is not the fastest way. If this is the case, taking this turn might even be a good option to evade possibly blocking situations.
	\item For the next grid tile, the agent should be able to detect if it is a switch and if so, if it is one the agent can make a decision on. (see \autoref{flatland_intro} for non-usable switches).
\end{itemize}
The provided TreeObsForRailEnv produce a graph with a node for each section of the rail. We extend these nodes with additional information about train traffic coming the other direction than the one the agent is heading. We take the information from these nodes and convert them into a numeric vector. In case that there is a dead end, we fill the observation with zeros (this could only happen in Flatland round 1, round 2 does not have dead ends). After the conversion, we concatenate all section observations into one large vector with information for all upcoming sections as illustrated in \autoref{fig:obs_descr}.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{diagrams/tree_obs_mapping.pdf}
	\caption{Illustration of the tree observation mapped onto the Flatland enviornment. Each colored bar represents information about a section.}
	\label{fig:obs_descr}
\end{figure}
While this vector already contains all required information outside of the agent, we add another vector with information regarding the agent itself (train specific observation in \autoref{obs_descr}). This vector contains the speed of the agent, the max. speed of the agent, the type of the current tile, the direction the agent is heading and 

\section{Technical Implementation Aspects}\label{software}
In this section, we discuss the most important implementation aspects of our work. 
\subsection*{Used Frameworks and Libraries}\label{framework_and_libraries}
In \autoref{table:used_technologies}, we list the most important used libraries and frameworks in this work.
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{ |l|c|X| } 
		\hline
		\textbf{Technology}	& \textbf{Version} & \textbf{Description}\\
		\hline
		Python & 3.6 & The whole code for our project is written in Python.\\
		\hline
		Tensorflow & 2.0.0 & Tensorflow provides a framework for both deep learning as well as monitoring the learning process of RL using Tensorboard.\\
		\hline
		MessagegPack & 0.6.2 & MessagePack allows very fast serialization. The resulting binary data is also considerably smaller than serialization with formats like JSON. We use MessagePack to serialize both weights- and gradient-update before sending between worker and model server.\\
		\hline
		Cython & 0.29.14 & Cython allows to compile Python code to natively running C code. We use Cython to speed up our observations on the rail environment.\\
		\hline
		Flask & 1.1.1 & We use Flask as our central model server (see \nameref{dist_architecture}). This Flask application collects gradient updates, distributes the current network weights and allows access to the latest versions of the observation and parameter files.\\
		\hline
		ZLib & (built-in) & ZLib allows to compress in Python. We use ZLib to reduce our payload-size while sending gradient- and weight updates.\\
		\hline
	\end{tabularx}
	\caption{The most important technologies used in this work.}
	\label{table:used_technologies}
\end{table}
\chapter{Experiment Design and Analysis}
\label{chap.experiment}
\chaptermark{Experiments}

\section[Reproducibility]{Reproducibility and Experiment Setup}\label{reproducability}
It is important to note, that the training process of reinforcement learning and especially multi agent reinforcement learning can be hard to reproduce. Depending on the initial weights of the neural networks and the layout of the environments, the performance may vary on each restart. Also in a distributed algorithm, the number of workers can significantly influence the training performance. If not differently specified, we execute all presented experiments on machines with the same specifications (see \autoref{infrastructure}).\\
Another aspect that is hard to reproduce is training stability. In A3C, an important instrument to prevent the policy from converging too early is using an additional entropy term \cite{a3c}. Our way to maintain stability with changing environments is discussed in \autoref{entropy_balancing_hyperparameter}.\\
For better comparability, we try to keep our evaluation versions for the training as similar to each other as possible. If not differently specified, we run evaluations with the following setup:
\begin{itemize}
	\item We use an environment of the size 100x100 tiles with 14 individual agents.
	\item The map contains 20 cities
	\item There is a maximum of 3 rails between cities and a maximum of 4 rails inside cities
	\item Based on the Flatland specification, the maximum allowed number of timesteps is 1608 \cite{flatland_spec}.
\end{itemize}
To analyze the performance of the solution, we run an analyzer that executes 20 rollouts of the environment using the same neural network parameters. After the these 20 rollouts, we update the neural network parameters. For each evaluation round, we use the same 20 environment layouts. 
To compare the performance in a graph, we now take the mean number of agents arrived at their target and plot that in our graph.
\section{Reinforcement Learning for Flatland}
\sectionmark{RL for Flatland}
\label{rl_flatland}
\subsection*{Action Space Reduction and Script Policy Actions}\label{reduced_action_space}
The Flatland environment is designed in a way to resemble a classical RL environment. This means, on every timestep, we receive observations for each agent, calculate an action and hand this action to the environment, visible in pseudocode \autoref{alg:no_reduction}.\\\\
\begin{algorithm}[H]
	\KwData{initialized Flatland environment $\mathcal{E}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents }
	\KwResult{terminal Flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
			\textit{// Fetch action for agent, based on current state\\}
			$\mathcal{A[a]} \leftarrow$ from policy $\pi (\mathcal{s\SPSB{a}{t}})$\\
		}
		
		call $\mathcal{env.step(A)}$ \\
		retrieve reward $\mathcal{R}$\\
		append $\mathcal{A}$ to buffer $\mathcal{B}$\\
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}

	use buffer $\mathcal{B}$ for training of policy $\pi$
	\caption{Default episode for Flatland environment}
	\label{alg:no_reduction}
\end{algorithm}
While this makes sense in an environment where agents need to take an action on every timestep (such as Atari games), in Flatland most of the time the only reasonable action is to move forward as visible in \autoref{fig:no_desicion}. Only around switches, an agent needs to take actions other than just keep going forward.
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{images/screenshot_no_decision.png}
	\caption{Screenshot from Flatland environment. A train heading east. The only reasonable action is to ride forward.}
	\label{fig:no_desicion}
\end{figure}
Every action that is produced by the neural network should be included for training, so the network can adapt to this type of situation. The problem arises now, that all these actions of riding forward are included into the training of the agent. The influence of the actions that actually matter (e.g. the ones around switches) is thereby not as big as it could be, because a large portion of the training data is actually situations that do not actually require decisions.\\
To solve this problem, we implement hard coded rules that the agents follow as long as they are not in a situation to make a decision. Only around switches, the neural network policy is activated. As a consequence, the data used for training has less samples but the samples available are of higher quality. The training with this mechanism is shown in \autoref{alg:with_reduction}.
For training, we only use the experience collected near the switch.\\
\begin{algorithm}[H]
	\KwData{initialized Flatland environment $\mathcal{env}$, initial observation $\mathcal{s\SPSB{a}{t=0}}$ for all agents}
	\KwResult{terminal Flatland environment}
	initialize buffer $\mathcal{B}$\\
	\While{episode not terminal}{
		create empty action array $\mathcal{A}$\\
		\For{every agent $\mathcal{a}$}{
			\eIf{agent is near to a switch}{
				get current state $\mathcal{s\SPSB{a}{t}}$ of agent\\
				\textit{// Fetch action for agent, based on current state\\}
				$\mathcal{A[a]} \leftarrow$ from policy $\pi (\mathcal{s\SPSB{a}{t}})$\\
			}{
				$\mathcal{A[a]} \leftarrow \mathcal{u_{forward})}$
			}
		}
		call $\mathcal{env.step(A)}$ \\
		retrieve reward $\mathcal{R}$\\
		\If{agent was near switch}{
			append $\mathcal{s_{t}}$, $\mathcal{A}$ and $\mathcal{R}$ to buffer $\mathcal{B}$\\
		}
		retrieve all new states $\mathcal{s_{t+1}}$\\		
	}
	use buffer $\mathcal{B}$ for training of policy $\pi$\\
	
	\caption{Improved learning algorithm for Flatland environment}
	\label{alg:with_reduction}
\end{algorithm}
This drastically improves training performance as visible in \autoref{chart:action_reduction_comparison}. While both versions have the potential to perform well, based on the observation data, we observe that the version without the action reduction is not able to performe nearly as well as the improved version with reduced actions.
\begin{figure}[H]
	\begin{center}
		\input{diagrams/comparison_action_reduction.pgf}
	\end{center}
	\caption{Comparison of training with and without action reduction}
	\label{chart:action_reduction_comparison}
\end{figure}
As a consequence, we keep the reduced action mechanism as part of our final solution.
\subsection*{Neural Network Architecture}\label{network_architecture}
In RL, the architecture of neural networks is often rather simple \cite{mnih2013playing}\cite{a3c}. The already sample inefficient process of reinforcement learning should not be slowed down by a difficult architecture. Differently than S. Huschauer in \cite{flatlandstephan}, we do not use a convolutional neural network. The reason for this is the way our observation vector is composed (see \autoref{enhanced_observations}). In this vector, every element corresponds with an actual information in the mapped rail section. Convolutional layers would especially make sense if there were patterns to extract from the observation vector.\\
Another more relevant aspect of neural networks is the topic of recurrent layers. Recurrent layers allow the agent to remember information from previous timesteps. The original A3C publication shows, that it is possible to achieve a significant performance improvement using long short-term memory layers (LSTM) \cite{a3c}. Also S. Huschauer uses an LSTM-block to improve training. To quantify this improvement, we run an experiment in \autoref{chart:lstm_comparison} to compare a version with recurrent layer to a version without.

\begin{figure}[H]
	\begin{center}
		\input{diagrams/lstm_comparison.pgf}
	\end{center}
	\caption{Comparison of training performance between a version with an LSTM-layer and a version without. It is evident that the version with LSTM performs better than the version without.}
	\label{chart:lstm_comparison}
\end{figure}

It is clearly visible that an LSTM-layer helps the training process and we therefore keep the LSTM-layer as part of our solution.

\subsection*{Curriculum Learning and Reward assignment}\label{curriculum_learning}
The reward assignment in Flatland can be freely configured. But as long as there is not some distance-to-target dependent reward function, the probability, that an agent with an uninformed policy finds its target is small. This is especially the case for large environments with many trains on it. For example, most evaluation environments of Flatland round 2 have up to 200 individual agents and are up 100x100 tiles large (SOURCE!). The rollout of such an environment takes a lot more time than the rollout of a 20x20 environment with 5 trains. Also, the probability, that a train arrives in a small environment is larger and therefore, the experience is more valuable for training.
To improve training times, it makes therefore sense to start with a small environment and move to larget ones, once the agents mastered pathfinding and basic collision avoidance.
In order to verify the meaningfulness of such a curriculum, we run an experiment with a large environment with dimensions 100x100 and 50 individual agents. One experiment tries to learn its policy directly using this environment. The other experiment uses a curriculum that gradually gets more difficult.
As visible in \autoref{chart:curriculum_comparison}, the version without curriculum is not able to learn from the experience. We suspect that is is caused by too much variance in the environment and not enough successful experience to learn from. After all, an agent needs to reach its target to get reward, and in a large environment combined with an uninformed policy, this is potentially very difficult.
\begin{figure}[H]
	\begin{center}
		\input{diagrams/curriculum_comparison.pgf}
	\end{center}
	\caption{Comparison of training performance between a version with and a version without curriculum learning. It appears, that the version without curriculum is not able to generalize well in larger environments.}
	\label{chart:curriculum_comparison}
\end{figure}
\begin{table}
	\centering
	\begin{tabular}{ |l|c|c|c|c|c| } 
		\hline
		& \textbf{Level 0} 
		& \textbf{Level 1}  
		& \textbf{Level 2} 
		& \textbf{Level 3} 
		& \textbf{Level 4}\\
		\hline
		\textbf{Next level on successrate} & 70\% & 70\% & 75\% & 70\% & 60\% \\  
		\textbf{Nr. of agent} & 4 & 8 & 12 & 16 & 20 \\  
		\textbf{Env. size} & 25x25 & 30x30  & 40x40 & 50x50 & 50x50 \\ 
		\textbf{Num. cities} & 5 & 8 & 10  & 12 & 16 \\ 
		\textbf{Max. rails between cities} & 1 & 2 & 2 & 2 & 2  \\ 
		\textbf{Max. rails in city} & 2 & 2 & 3 & 3 & 3  \\ 
		\hline
	\end{tabular}
	\label{table:curriculum_data}
	\caption{Curriculum level specifications for our experiment to compare a version with curriculum to a version without.}
\end{table}
While it is clearly visible in \autoref{chart:curriculum_comparison} that directly training on difficult levels is not feasible, it also appears that the learning speed is slowed down on higher curriculum levels. With increasingly more difficult levels and more agents, the frequency of available network updates is being slowed down due to the fact that a rollout of the environment takes longer. We therefore think, it is desirable to keep the curriculum levels small and primarily increase the number of agents instead of the size of the environment.


\subsection*{Agent Communication}\label{agent_communication}
Communication in multi agent reinforcement learning is a topic of active research (SOURCE)) While we implement only very limited communication in this work, in this section we discuss a number of ideas regarding the use of communication in the Flatland environment. As a starting point for this work we asked the question how blocking is avoided in the real world. On the SBB rail environment railroad signals are used to prevent collisions between trains. These signals are controlled by a central control instance that directs the flow of traffic for all trains on the grid.
The Flatland environment does not provide a communication channel nor a rail road signal system, but participants of the challenge are allowed to implement one themselfes. In the following discussion, we summarize available options to implement such a communication channel. An initial intuition for the problem gives the question, if it was a human guiding the train, would she/he be able to successfully steer the train to its assigend target without communication?
We assume that this is not the case. While classical traffic rules (e.g. right has priority over left) can help to avoid collisions, we do think that in the case of railroad traffic this does not suffice. In the real world, rail traffic is controlled by a control center that gives the trains permission to go or tells them to wait. Having one central control instances solves one of the big problems of such a system which is, that there is a chance that two agents take an action at the same moment which does bring both of them into a non-resolvable situation. An example for such a situation would be two agents that enter a rail section at the same timestep. Both of them would have had the possibility to choose a different path, but both perceived the section as empty and decided to enter. Combined with the fact that trains cannot drive backwards (at least in Flatland), both trains will not be able to arrive at their assigned targets. As a solution to this dilemma, we discuss four ideas that could potentially improve the situation.
\begin{itemize}
	\item \textbf{Negotiation:} To solve the problem of conflicting actions at the same timestep, it would be possible to introduce a iterative communication channel, on which the agents can negotiate, what agent is allowed to take an action next and who has to wait another timestep. The number of iterations could be determined by the outcome of the process. As long as there is no resolution about who is allowed to take an action and who is not, another negotiation round is added.\\
	While this procedure might help to avoid blocking situations, it could certainly not completely remove them. Especially complex situations with many agents in a small area could still prove to be difficult, even with a negotiation mechanism in place.

	\item \textbf{Prioritized planning:} An approach to solve the dilemma of conflicting actions at the same timestep could be to introduce an artifical order of importance among the agents. Then the next $\mathcal{n}$ timesteps could be planned for the most important agent. The second agent now takes the planning of the first agent into account and tries to come up with a plan that does not obstruct the planned route of the first agent. This process is repeated for all following agents.\\
	We think that possible conflicts could maybe be resolved by backtracking and adjusting the priorities of the trains.	
	
	\item \textbf{Unconstrained communication:} While negotiation would prevent an agent from taking an action, the goal of unconstrained actions would not be to constrain the action space but rather to convince the agent to choose an action \textit{stop} or \textit{do nothing} and wait for the next timestep if the situation is uncertain. Also for unconstrained actions, it would make sense to be an iterative process, similar to the negotiation approach. As long as not all agents mark their communication as completed, another communication round is added.\\
	We suspect that a problem with this approach could be that it would have much slower convergence than the negotiation approach. The reason for this assumption is the fact, that such an unconstrained communication would not directly influcence the actions of the agents. It would be nescessary to learn both the "speaking" and the "interpretation", without a direct consequences. On the positive side, we think that this approach could enable more sophisticated strategies and maybe even improve planning.
	
	\item \textbf{Announcing communication:} Differently than unconstrained communication, under \textit{announcing} communication we mean agents that announce their next action over a shared communication channel. As a metaphor, one can imagine a bus on a narrow mountain street honking before each turn. A mechanism like this would, similar to \textit{prioritized planning} require to receive the agent actions in a sequential way, so that each following agent could react to the actions of the previous agents.
\end{itemize}

We think that the two areas of prioritized planning and unconstrained communication would have the biggest potential to improve train behaviour in our Flatland solution. In this work, we do not consider to implement planning and therefore focus on the case of unconstrained communication. We argue, that a communication channel that does not provide a defined protocol could eventually also have the benefit of allowing the agents to plan ahead.\\\\
\textit{Experiment Setup}\\\\
To verify the assumption that it is possible to learn a communication protocol to plan ahead, we create an experiment that reduces the observation space to a shared communication buffer. For this experiment we defined a Flatland layout with two train stations and only one rail that connects them. We let a train start in each train station and assign the opposite station as its target. We also add two detour tracks that allow the agents to evade a collision with the agent (REF TO IMAGE).
The agents only need to take an action on switches they encounter, otherwise they just drive forward. The agents can not observe the upcoming sections and only have access to the said buffer. Due to the symmetry of the environment, both agents need to take an action at the same time. The agents can select an action between 0 and 5. On taking an action, we start a communication loop that allows the two agents to alternately read from the buffer, calculate an action and write that action back into the buffer. This loop continues until both agents output the action 5 (= we are done with communication). After finishing communication, both agents can select an action to take next in the environment. If both agents take the same action and collide, we cancel the episode and give a reward of -1. If they make it around each other and reach their targets, they receive a reward of +1. The agents have no way to know, if they are agent 1 or 2 (otherwise, agent 1 could just always make a loop and agent 2 could go straight).\\\\
\textit{Experiment Analysis}\\\\
We observe, that the agents are able to learn a protocol themselves. After 100'000 episodes of training, the succesrate to fulfil the task is as high as 95\%. Previously to the experiment, we assumed, that if the agents would learn a way of communication, it would probably look the same for every episode. This assumption has not been confirmed. In all observed cases, the agents need between 2 and 5 communication timesteps. In \autoref{table:communication_samples}, we list 6 examples of observed communications with their regarding outcomes.
\begin{table}[H]
	\centering
	\begin{tabular}{ |c|c|c|c|c|c| } 
		\hline
		\textbf{Timestep} 
		& \textbf{Actions agent 1|2} 
		& \textbf{Outcome}\\
		\hline
		0 & 4 | 2 &\\
		1 & 5 | 5 & Success\\
		\hline
		0 & 3 | 0 &\\
		1 & 1 | 5 &\\
		2 & 5 | 5 & Success\\
		\hline
		0 & 3 | 5 &\\
		1 & 5 | 5 & Success\\
		\hline
		0 & 3 | 1 &\\
		1 & 3 | 2 &\\
		2 & 5 | 0 &\\
		3 & 5 | 5 & Crash\\
		\hline
		0 & 3 | 2 &\\
		1 & 5 | 3 &\\
		2 & 5 | 4 &\\
		3 & 2 | 5 &\\
		4 & 5 | 5 & Success\\
		\hline
		0 & 4 | 3 &\\
		1 & 3 | 1 &\\
		2 & 5 | 5 & Success\\
		\hline
	\end{tabular}
	\label{table:communication_samples}
	\caption{Examples of communication in our agent communication experiment.}
\end{table}
While we are surprised that the communication does not look the same on every episode, we think it is remarkable how appearantly a language between the agents emerge. Due to reward discounting, it would make sense to have as few communication steps as possible. Still, the agents seem to decide, that it is more valueable to add more communication steps to reach an agreement, instead of going straight to the end of communication.\\
It is also interresting, that agent 2 never seems to respond with the same action as agent 1. The question, if such a behaviour could be generalized in less constrained environment is discussed in \nameref{chap.diskussion}.
\section{Distributed Architecture and Parallelism}\label{dist_architecture}
\subsection*{Distributed training}
One of the main advantages of the A3C algorithm is its ability to be used in a distributed manner. This allows to run multiple versions of the environment asynchronously and collect the updates for both the actor and the critic network in a central place. To fully take advantage of this mechanism, we implemented a system that allows to run a large number of environements at the same time. All running training instances contribute to the same central network. Thanks to HTTP-based connections, we can use this mechanism even between computers or even networks. This approach is only limited by the capability of the central model server and the network throughput.\\
Additionally, our central model server not only handles the update of the neural network but also distributes both the code for building observations and a file with all hyperparameter required for training on startup. The startup process of the worker node then converts the code for building observations into native C-code using Cython. This converted code then gets compiled and dynamically imported as a Python module.\\
Using native C-code speeds up the rollout of the environment and therefore the training process by a factor of 2 to 5.
\begin{figure}[H]
	\centering
	\includegraphics[width=300pt]{diagrams/distributed_training_update.pdf}
	\caption{Simplified procedure of training of a single worker process. The communication between the worker and the model server works by using HTTP. The compilation of the observation.pyx file is only done once on each machine.}
	\label{obs_descr} %TODO: change
\end{figure}
To improve the performance of the system, we compress both the weights (server to worker) and the gradient update (worker to server) using ZLIB. This reduces the size of the transmitted data between 10\% to 80\%.
\begin{figure}[H]
	\centering
	\includegraphics[width=400pt]{images/visio/architecture.png}
	\caption{Illustration of distributed architectures with main node (node 1) and several additional nodes (2-w) connected over HTTP.}
	\label{dist_architecture_img}
\end{figure}
To analyze the impact of training a policy with multiple workers a the same time, we run an experiment that compares having a single worker to training with 7 workers at the same time.
While it is appearant in \autoref{chart:singlemany_comparison} that having mulitple workers does speed up the training process, we do not observe any impact of the number of workers on the quality of the policy.
\begin{figure}
	\begin{center}
		\input{diagrams/singlemany_comparison.pgf}
	\end{center}
	\caption{Comparison of training with 1 worker and with 7 workers}
	\label{chart:singlemany_comparison}
\end{figure}
\subsection*{Infrastructure}\label{infrastructure}
The infrastructure used for this project consists of 4 machines used in various combinations.
\begin{itemize}
	\item \textbf{Draft Animal (DRAN):} Server with 56 CPUS and 721 GB of RAM. This machine is mainly used to train the current submission model.
	\item \textbf{Openstack machines:} 3 servers with 8 CPUS each. One machine has 16 GB of RAM, the other two have 64 GB each.
\end{itemize}
The A3C algorithm is not able to take advantage of GPUs. The reason for this is fragmented update process. Differently than with an experience replay buffer, the training data is only used once and gets then discarded.
\newpage



