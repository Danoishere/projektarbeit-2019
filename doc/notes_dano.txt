- What we did -
---------------

Basic a3c learning environment
TreeObservation ausprobiert keine grossen erfolge
GlobalObservation + Conv-Network
Early stopping eingeführt

Problem: Rewards sehr spärlich. Idee: Wir erlösen Agent von step-penalty, wenn 
    dieser Schritt in die richtung des Ziels des Agents ist.
    Reward für annäherung an Target eingeführt


Residual Connection eingeführt
Oftmals Problem: Nach einer Weile wählt Programm immer gleiche Aktion
Besonders bei "Nicht-Debugging-Modus"

Mass: Anz. Agents die Ziel erreicht haben/min

Überlegungen:
    Ja observation: POMDP -> Markov-Eigentschaft nicht erfüllt! 
    Ausser evtl. mit Globalen Observation

    Reward nur wenn ALLE Agents das Ziel erreichen?
    Versuch: 
    Erhaltener Reward steigt mit Nummer des Agents.
    Plus Bonus für alle Agents

    Negativ-Reward bei Blockade?


Idee: Kommunikation zwischen Agenten
-> Wie? bzw. wie trainieren
-> Problem: Einfach sicht der anderen Agenten übergeben reicht wohl nicht.

Vergleiche:
https://pub.tik.ee.ethz.ch/students/2018-FS/BA-2018-04.pdf


# OpenAI - A2C baseline
https://github.com/openai/baselines/blob/master/baselines/a2c/a2c.py

# Metriken:
Reward pro Testparkour
Finished Agents pro Testparkour
Durchschnittliche Ankunftsrate bei 1|2|3... Agents

